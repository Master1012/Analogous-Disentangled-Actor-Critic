D:\nlp\torchenv\lib\site-packages\gymnasium\envs\registration.py:517: DeprecationWarning: [33mWARN: The environment InvertedPendulum-v4 is out of date. You should consider upgrading to version `v5`.
  logger.deprecation(
Warning: Log dir E:\Analogous-Disentangled-Actor-Critic\agents\data\InvertedPendulum-v4\InvertedPendulum-v4_s0 already exists! Storing info there anyway.
[32mLogging data to E:\Analogous-Disentangled-Actor-Critic\agents\data\InvertedPendulum-v4\InvertedPendulum-v4_s0\progress.txt
[36mSaving config:
{
    "exp_name":	"InvertedPendulum-v4",
    "logger":	{
        "<spinup.utils.logx.EpochLogger object at 0x0000024D58BE2380>":	{
            "epoch_dict":	{},
            "exp_name":	"InvertedPendulum-v4",
            "first_row":	true,
            "log_current_row":	{},
            "log_headers":	[],
            "output_dir":	"E:\\Analogous-Disentangled-Actor-Critic\\agents\\data\\InvertedPendulum-v4\\InvertedPendulum-v4_s0",
            "output_file":	{
                "<_io.TextIOWrapper name='E:\\\\Analogous-Disentangled-Actor-Critic\\\\agents\\\\data\\\\InvertedPendulum-v4\\\\InvertedPendulum-v4_s0\\\\progress.txt' mode='w' encoding='cp1252'>":	{
                    "mode":	"w"
                }
            }
        }
    },
    "self":	{
        "<agents.sac.SAC object at 0x0000024D50FF0EE0>":	{
            "ac_kwargs":	{},
            "actor_critic":	"MLPActorCritic",
            "alpha":	0.2,
            "batch_size":	100,
            "device":	"cuda",
            "env_fn":	"<function SAC.__init__.<locals>.<lambda> at 0x0000024D5216CCA0>",
            "epochs":	100,
            "gamma":	0.99,
            "logger_kwargs":	{
                "exp_name":	"InvertedPendulum-v4",
                "output_dir":	"E:\\Analogous-Disentangled-Actor-Critic\\agents\\data\\InvertedPendulum-v4\\InvertedPendulum-v4_s0"
            },
            "lr":	0.001,
            "max_ep_len":	1000,
            "num_test_episodes":	10,
            "polyak":	0.995,
            "replay_size":	1000000,
            "save_freq":	1,
            "seed":	0,
            "start_steps":	10000,
            "steps_per_epoch":	4000,
            "update_after":	1000,
            "update_every":	50
        }
    }
}
[32mNumber of parameters: 	 pi: 67586, 	 q1: 67585, 	 q2: 67585
tensor([[-3.6877e-02, -1.6017e-01,  1.9768e-02, -1.0521e+00],
        [-9.2077e-02,  1.3383e-01,  1.7185e-01, -4.2470e-01],
        [-5.0627e-02,  1.0181e-01, -1.3810e+00,  3.1347e+00],
        [ 5.5166e-04,  1.3553e-02, -6.1463e-01,  1.4447e+00],
        [ 2.6940e-02, -6.4595e-02, -3.1954e-01,  7.7909e-01],
        [-1.3028e-03, -6.4690e-03,  2.9739e-03,  6.2435e-03],
        [-7.5777e-02,  1.8432e-01, -4.2344e-01,  1.1979e+00],
        [ 2.3261e-02, -7.0384e-02,  2.4933e-01, -4.5207e-01],
        [-6.3424e-03,  2.2918e-02, -6.3592e-01,  1.4687e+00],
        [ 6.4187e-03, -3.5684e-03, -5.8134e-01,  1.3697e+00],
        [ 5.5772e-02, -1.0940e-01,  1.1432e+00, -2.5765e+00],
        [-2.6336e-02,  6.2285e-02, -3.9177e-01,  8.6497e-01],
        [-1.8068e-02,  2.3350e-02, -8.0693e-01,  1.8431e+00],
        [ 8.1652e-03,  5.2285e-03,  1.3784e+00, -2.6632e+00],
        [ 2.2666e-02, -2.6575e-03,  9.9054e-01, -2.0198e+00],
        [ 9.1499e-03, -2.9722e-02,  4.1640e-01, -9.7925e-01],
        [ 4.7471e-02, -6.2006e-02,  5.9129e-01, -7.3977e-01],
        [ 5.9623e-02, -7.0597e-02,  1.5032e+00, -2.7870e+00],
        [-2.2734e-02,  4.7865e-02,  1.7621e-01, -4.2321e-01],
        [-5.0643e-02,  1.1638e-01, -5.1380e-01,  1.2375e+00],
        [-7.6089e-02,  1.7130e-01, -8.0214e-01,  1.8782e+00],
        [ 4.2599e-02, -1.0214e-01, -2.8202e-01,  6.4035e-01],
        [-4.1126e-02,  9.6839e-02, -9.4230e-01,  2.1285e+00],
        [ 5.7096e-03, -1.6619e-02,  2.8523e-01, -6.5239e-01],
        [ 4.7471e-02, -6.2006e-02,  5.9129e-01, -7.3977e-01],
        [ 3.2442e-04, -8.2573e-03, -4.7957e-01,  1.1321e+00],
        [ 6.1954e-02, -1.5695e-01, -3.2709e-01,  3.3105e-01],
        [ 3.4096e-03,  9.1851e-03, -2.3153e-04, -1.8045e-03],
        [-3.0481e-03,  3.5346e-02, -5.3851e-01,  1.2565e+00],
        [ 9.1497e-02, -1.3510e-01,  1.2659e+00, -2.5707e+00],
        [-8.8532e-02,  2.0000e-01, -1.1137e+00,  2.5077e+00],
        [ 4.3743e-03, -1.5950e-02,  5.4228e-01, -1.2220e+00],
        [-9.8874e-03,  6.9089e-03,  2.8627e-03,  1.7241e-03],
        [-3.6271e-02,  4.7029e-02, -4.6243e-01,  9.4875e-01],
        [-4.5721e-02,  1.0574e-01, -5.7746e-01,  1.3096e+00],
        [ 1.7907e-02, -1.1490e-02,  4.6791e-01, -1.0476e+00],
        [-6.1145e-03,  5.4824e-03, -2.0740e-03,  2.0970e-03],
        [ 8.3083e-03, -1.7055e-03,  3.8608e-02, -8.0009e-02],
        [-2.3569e-03,  6.7243e-04,  2.6619e-03,  1.7443e-03],
        [-3.8445e-03,  1.1157e-03,  2.6821e-01, -6.3678e-01],
        [ 8.9196e-03, -8.5763e-03,  1.3761e-01, -3.3445e-01],
        [-4.8532e-02,  1.2785e-01,  5.6158e-01, -6.3749e-01],
        [ 7.1139e-02, -1.6454e-01, -8.7391e-02,  1.6894e-01],
        [-4.5721e-02,  1.0574e-01, -5.7746e-01,  1.3096e+00],
        [ 1.5036e-02, -3.6566e-02, -8.7780e-01,  1.7221e+00],
        [-2.7303e-02,  8.3588e-02, -9.6366e-01,  2.2096e+00],
        [ 1.7921e-02, -1.3037e-02,  4.2336e-01, -9.7174e-01],
        [-1.2126e-02,  3.2120e-02, -5.8177e-01,  1.3459e+00],
        [-1.4146e-02,  5.2491e-03, -1.4707e-01,  2.9643e-01],
        [ 6.0178e-03,  1.4171e-02,  5.1185e-01, -1.1621e+00],
        [ 2.6562e-02, -8.2875e-02,  2.4498e-01, -5.2268e-01],
        [ 1.2518e-02, -3.2210e-02,  5.8328e-01, -1.3288e+00],
        [-1.1365e-02,  3.0223e-02, -4.6064e-01,  1.0690e+00],
        [ 1.4870e-02, -1.9053e-02, -2.8964e-01,  6.5476e-01],
        [-2.1484e-02,  3.8163e-02, -8.4277e-01,  1.9104e+00],
        [-4.4470e-03, -2.8538e-04, -6.6961e-03, -7.8733e-03],
        [ 2.2261e-02, -3.0664e-02,  6.5553e-01, -1.5201e+00],
        [-1.7730e-03, -9.2293e-04, -6.9335e-03,  5.3085e-03],
        [-2.1494e-02,  2.2261e-02, -6.8667e-01,  1.5463e+00],
        [ 5.6083e-02, -1.1694e-01, -3.9662e-01,  6.8470e-01],
        [ 5.3286e-02, -1.2152e-01,  1.2505e+00, -2.6285e+00],
        [ 8.4737e-03, -8.1844e-03,  6.3972e-03,  1.6154e-03],
        [ 1.3470e-02, -1.9752e-02,  3.0198e-01, -7.0901e-01],
        [-2.1297e-02,  3.4540e-02, -9.3149e-01,  2.1279e+00],
        [ 3.7013e-03,  9.0684e-03,  2.1803e-03, -6.4243e-03],
        [-7.3539e-03, -3.7686e-04, -6.7732e-03, -2.1527e-03],
        [-2.6218e-02,  4.9247e-02, -8.7648e-03, -5.5434e-03],
        [-2.6218e-02,  4.9247e-02, -8.7648e-03, -5.5434e-03],
        [ 2.4475e-02, -6.4977e-02,  4.0735e-01, -7.4235e-01],
        [ 4.5011e-03,  4.6366e-03, -6.2217e-03, -1.5233e-03],
        [-7.0106e-03,  3.1416e-03, -2.4785e-01,  5.6707e-01],
        [ 3.9248e-02, -9.5584e-02,  4.5233e-01, -9.9268e-01],
        [-8.4463e-04,  1.5934e-02, -3.1226e-01,  7.2880e-01],
        [-5.6234e-02,  1.2992e-01,  4.8884e-02, -7.5045e-02],
        [ 1.3718e-02, -5.3807e-02,  9.9015e-01, -2.2604e+00],
        [-1.6238e-02,  5.9584e-02,  1.5212e-01, -2.0570e-02],
        [ 7.5082e-03,  2.6992e-03, -4.8530e-03,  8.1831e-03],
        [-2.8759e-02,  4.9003e-02, -9.4738e-01,  2.1854e+00],
        [-2.7929e-02,  7.3435e-02, -7.6668e-03,  8.7737e-02],
        [ 5.7475e-03, -5.1550e-03, -1.8425e-03,  7.8915e-03],
        [-4.4470e-03, -2.8538e-04, -6.6961e-03, -7.8733e-03],
        [ 5.4864e-03, -7.1100e-03, -1.2389e-03,  1.1030e-03],
        [-4.1929e-02,  8.5117e-02, -4.8931e-01,  1.0335e+00],
        [ 3.1501e-02, -6.3408e-02,  7.8071e-01, -1.8003e+00],
        [-4.3161e-03,  3.6767e-04,  1.8896e-01, -4.0503e-01],
        [-4.7140e-03,  1.1348e-02,  4.2504e-01, -1.0172e+00],
        [-1.1602e-02,  4.2162e-02,  4.8216e-03,  6.3868e-02],
        [ 2.0349e-02, -1.0267e-02, -5.4350e-01,  1.2814e+00],
        [-6.0524e-02,  1.4758e-01,  9.1355e-02, -9.5445e-02],
        [-3.6009e-03, -6.0176e-03,  1.0801e-03,  5.9450e-03],
        [-8.8489e-03,  1.2277e-02, -1.5471e-01,  3.5351e-01],
        [ 7.3999e-03,  8.5314e-05,  6.7360e-03, -8.8857e-03],
        [ 1.3936e-02, -2.1201e-02,  5.1619e-01, -1.1932e+00],
        [ 2.8841e-02, -6.3173e-02,  8.1052e-01, -1.8553e+00],
        [ 6.5125e-03, -4.3086e-02,  8.2009e-01, -1.8840e+00],
        [ 1.1301e-01, -1.3870e-01, -1.5860e-01,  9.5820e-01],
        [-8.1248e-02,  1.6888e-01,  1.4752e-01, -2.5456e-01],
        [ 3.2371e-02, -7.1103e-02,  9.9803e-01, -2.2551e+00],
        [-6.9846e-02,  1.3846e-01, -1.0252e+00,  2.2546e+00],
        [-1.2947e-02,  2.7112e-02, -7.7787e-01,  1.7389e+00]]) tensor([[-2.6716],
        [ 0.6617],
        [-2.4560],
        [-0.5101],
        [ 2.1861],
        [-1.3250],
        [-0.1773],
        [ 1.1906],
        [-1.8507],
        [-2.9677],
        [-0.0690],
        [-0.5587],
        [ 2.6734],
        [-2.3138],
        [-1.1563],
        [-2.2750],
        [-1.7320],
        [-2.4899],
        [ 2.8014],
        [ 1.0842],
        [-0.3838],
        [-1.3577],
        [ 1.1954],
        [ 1.4124],
        [-1.7320],
        [ 2.2291],
        [-2.1000],
        [-1.2764],
        [-1.4913],
        [ 1.6103],
        [ 2.0667],
        [ 0.4205],
        [-2.5069],
        [ 0.8080],
        [ 1.9049],
        [ 1.3115],
        [ 1.2229],
        [-2.0065],
        [-1.5865],
        [ 1.7566],
        [-2.7738],
        [-2.7287],
        [-0.1766],
        [ 1.9049],
        [-1.1368],
        [-1.6452],
        [ 1.9102],
        [ 2.2326],
        [ 0.7213],
        [ 0.1517],
        [ 2.5564],
        [-0.7531],
        [ 1.2678],
        [-2.2952],
        [-1.8408],
        [-2.5206],
        [ 1.5964],
        [-1.1509],
        [-2.9871],
        [ 2.6877],
        [-1.4723],
        [ 0.0606],
        [ 1.1207],
        [-0.0901],
        [ 2.5374],
        [ 0.4436],
        [-2.1106],
        [-2.1106],
        [-0.7651],
        [-2.0740],
        [ 0.7713],
        [-2.2312],
        [-1.1619],
        [ 1.6130],
        [-0.8835],
        [-0.3075],
        [-0.5099],
        [-2.4246],
        [ 1.8217],
        [-1.0737],
        [-2.5206],
        [ 0.4526],
        [ 2.3580],
        [-1.3999],
        [-0.5645],
        [ 2.6212],
        [-0.8107],
        [ 1.8383],
        [ 2.6232],
        [-2.2775],
        [ 0.5390],
        [ 0.0969],
        [ 2.1963],
        [ 1.5239],
        [-1.4898],
        [-0.5661],
        [ 1.3529],
        [ 2.3042],
        [ 0.2116],
        [ 1.4345]])
Traceback (most recent call last):
  File "E:\Analogous-Disentangled-Actor-Critic\main.py", line 141, in <module>
    main()
  File "E:\Analogous-Disentangled-Actor-Critic\main.py", line 129, in main
    trainer = Trainer(args)
  File "E:\Analogous-Disentangled-Actor-Critic\trainers\Trainer.py", line 117, in __init__
    sac.sac()
  File "E:\Analogous-Disentangled-Actor-Critic\agents\sac.py", line 346, in sac
    update(data=batch)
  File "E:\Analogous-Disentangled-Actor-Critic\agents\sac.py", line 257, in update
    loss_q, q_info = compute_loss_q(data)
  File "E:\Analogous-Disentangled-Actor-Critic\agents\sac.py", line 206, in compute_loss_q
    q1 = ac.q1(o,a)
  File "D:\nlp\torchenv\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "E:\Analogous-Disentangled-Actor-Critic\./agents\spinup\sac\core.py", line 77, in forward
    q = self.q(torch.cat([obs, act], dim=-1))
  File "D:\nlp\torchenv\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\nlp\torchenv\lib\site-packages\torch\nn\modules\container.py", line 139, in forward
    input = module(input)
  File "D:\nlp\torchenv\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\nlp\torchenv\lib\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)